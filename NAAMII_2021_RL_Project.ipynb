{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAAMII 2021 - RL Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Supriya090/2021-naamii-rl-practical/blob/main/NAAMII_2021_RL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBvZlX0bK2qc"
      },
      "source": [
        "# 2021 NAAMII AI Winter School RL Practical\n",
        "Authors: John D. Martin (jmartin8@ualberta.ca), Shruti Mishra, Jordan Hoffmann"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment covers a number of important reinforcement learning concepts, algorithms, and phenomena. Students will observe the general interaction between a learning system and its environment. In addition, they will implement the classic algorithms SARSA and Q-Learning, for policy evaluation and control. Using experience in a small gridworld domain, students will observe how the presence of stochasticity influences step size sensitivity. The focus shifts in the latter part of the assignment to issues that arise under function approximation with a neural network. Using experience in a domain with continuous states and actions, students will observe how performance of the PPO algorithm changes with different architecture sizes.      \n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "y7KL4v193OYt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA7LDeADOkns"
      },
      "source": [
        "# Environment defintion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_XinzPNlSYw"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import functools\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7e4fUtYYdY_"
      },
      "source": [
        "#@title 5x5 GridWorld\n",
        "class GridWorld:  \n",
        "  def __init__(self, stochastic=False, seed=0):\n",
        "    self.stochastic = stochastic\n",
        "    self.seed = seed\n",
        "\n",
        "    self.num_rows = 5\n",
        "    self.num_cols = 5\n",
        "    self.num_actions = 4\n",
        "    self.num_states = self.num_cols * self.num_rows\n",
        "\n",
        "    self._rand = np.random.RandomState(self.seed)\n",
        "\n",
        "    self._init_topology()  \n",
        "    self.counts = np.zeros(self.num_states)\n",
        "\n",
        "  def _init_topology(self, initial_grid=None):    \n",
        "    # Assign the adjacency: [W, N, E, S]\n",
        "    self.adjacency_list = np.array(\n",
        "                          [[0,1,5,0],\n",
        "                           [1,2,6,0],\n",
        "                           [2,3,7,1],\n",
        "                           [3,4,8,2],\n",
        "                           [4,4,9,3],\n",
        "                           [0,6,10,5],\n",
        "                           [1,7,11,5],\n",
        "                           [2,8,12,6],\n",
        "                           [3,9,13,7],\n",
        "                           [4,9,14,8],\n",
        "                           [5,11,15,10],\n",
        "                           [6,12,16,10],\n",
        "                           [7,13,17,11],\n",
        "                           [8,14,18,12],\n",
        "                           [9,14,19,18],\n",
        "                           [10,16,20,15],\n",
        "                           [11,17,21,15],\n",
        "                           [12,18,22,16],\n",
        "                           [13,19,23,17],\n",
        "                           [14,19,24,18],\n",
        "                           [15,21,20,20],\n",
        "                           [16,22,21,20],\n",
        "                           [17,23,22,21],\n",
        "                           [18,24,23,22],\n",
        "                           [19,24,24,23]])\n",
        "    \n",
        "  def _get_obs(self):\n",
        "    return jax.nn.one_hot(self.state, self.num_states)\n",
        "\n",
        "  def _get_reward(self):\n",
        "    if self.state == self.bonus_idx:\n",
        "      return 1.\n",
        "    return 0.\n",
        "  \n",
        "  def _get_done(self):\n",
        "    if self.state == self.bonus_idx:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = 0\n",
        "    self.bonus_idx = self.num_states - 1\n",
        "    self.counts[self.state] += 1\n",
        "    return self._get_obs()\n",
        "\n",
        "  def step(self, action):\n",
        "    if self.stochastic:\n",
        "      if rs.rand() < 0.1:\n",
        "        action = rs.choice(self.num_actions)\n",
        "    self.state = self.adjacency_list[self.state, action]\n",
        "    self.counts[self.state] += 1\n",
        "    return self._get_obs(), self._get_reward(), self._get_done()\n",
        "\n",
        "  def get_xy(self, state):\n",
        "    state = state % (self.num_cols * self.num_rows)\n",
        "    x = state // self.num_cols\n",
        "    y = state % self.num_rows\n",
        "    return x, y\n",
        "  \n",
        "  def get_state_number(self, x, y):\n",
        "    return x * self.num_cols + y\n",
        "      \n",
        "  def plot(self):\n",
        "    fig, ax = plt.subplots()\n",
        "    grid = np.reshape(self.counts, (self.num_rows, self.num_cols))\n",
        "    for i in range(self.num_rows):\n",
        "      for j in range(self.num_cols):\n",
        "        #  ax.text(i+.5, j+.5, str(grid[i,j]),va='center', ha='center')\n",
        "        ax.text(i+.5, j+.5, str(self.get_state_number(i,j)),va='center', ha='center')\n",
        "    # ax.imshow(np.flip(grid.transpose(),0),\n",
        "    #           extent=[0, self.num_rows, 0, self.num_cols])\n",
        "    state_numbers = np.reshape(np.arange(self.num_states), (self.num_rows, self.num_cols))\n",
        "    ax.imshow(state_numbers,\n",
        "              extent=[0, self.num_rows, 0, self.num_cols])\n",
        "    i, j = self.get_xy(self.state)\n",
        "    #ax.text(i+.5, j+.5, 'x', )\n",
        "\n",
        "    i, j = self.get_xy(self.bonus_idx)\n",
        "    #ax.text(i+.5, j+.5, '+1', va='center', ha='center')\n",
        "\n",
        "    ax.grid(which='major', linestyle='--')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "fJfY5rbVxvom",
        "outputId": "219ff223-41ad-4281-f7ba-da0edfa1cb69"
      },
      "source": [
        "e = GridWorld()\n",
        "e.reset()\n",
        "e.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcDlDYnZVtSk"
      },
      "source": [
        "def compute_returns(rewards, discount, bin_size=1):\n",
        "    assert len(rewards) >= bin_size\n",
        "    discounts = [discount**t for t in range(bin_size)]    \n",
        "    return np.array([np.dot(rewards[t+1:t+1+bin_size], discounts)\n",
        "                     for t in range(len(rewards)-bin_size)])\n",
        "\n",
        "\n",
        "def bin_vector(x, num_bins):\n",
        "  j = int(len(x) / num_bins)\n",
        "  return np.array([np.sum(x[i * j:(i + 1) * j]) for i in range(num_bins)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SM8lZ7KVLhV"
      },
      "source": [
        "# Part 1. Decision policies\n",
        "\n",
        "In reinforcement learning a policy determines how the agent behaves. Mathematically, a policy is a mapping from states to actions $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$. This mapping can be either deterministic or stochastic. In many cases, and particularly for the algorithms we consider, the policy is defined in terms of the value function. To familarize yourself with these ideas, you will implement several well-known decision policies. \n",
        "\n",
        "1. A uniform random policy: $a \\sim \\text{Uniform}(\\mathcal{A})$. \n",
        "2. A greedy decision policy that breaks ties unifomly at random. Do this for Gaussian-random values. \n",
        "3. An epsilon-greedy policy that takes random actions $\\varepsilon$ percent of the time. Fix $\\varepsilon=0.1$ for this exercise.\n",
        "\n",
        "After many steps of experience, the distribution of state visitation counts will be proportional to the stationary distribution of the MDP. For each policy visualize the distribution of state visitation counts using the environment attribute env.counts. How do the three policies compare?\n",
        "\n",
        "For more information on polices and value functions see section 3.5 of Sutton and Barto. Within the same test, the greedy and epsilon-greedy policies are discussed in section 2.2. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Exercise: Implement decision policies.\n",
        "With the provided function signatures, implement the three decision policies mentioned above.  "
      ],
      "metadata": {
        "id": "t5m2gV2LIhpc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_hripYUVpOs"
      },
      "source": [
        "#@title Policies (Exercise)\n",
        "def random_action(actions, rs):\n",
        "  \"\"\"\n",
        "    actions: an np array of action indices.\n",
        "    rs: the np random state.\n",
        "  \"\"\" \n",
        "  # YOUR CODE GOES HERE.\n",
        "  return rs.choice(actions)\n",
        "\n",
        "\n",
        "def greedy_action(w, x, rs):\n",
        "  \"\"\"\n",
        "    w: a (num_states, num_actions) jnp matrix of action values.\n",
        "    x: observation vector.\n",
        "    rs: the np random state.\n",
        "  \"\"\" \n",
        "  # YOUR CODE GOES HERE.\n",
        "  q = jnp.matmul(x,w)\n",
        "  a = jnp.flatnonzero(q == jnp.max(q))\n",
        "  return a[0]\n",
        "\n",
        "\n",
        "def epsilon_greedy(w, x, rs, epsilon=0.1):\n",
        "  \"\"\"\n",
        "    w: a (num_states, num_actions) jnp matrix of action values.\n",
        "    x: observation vector.\n",
        "    rs: the np random state.\n",
        "    epsilon: the probability of taking a random action.\n",
        "  \"\"\" \n",
        "  # YOUR CODE GOES HERE.\n",
        "  if rs.rand() < epsilon:\n",
        "    return rs.choice(jnp.shape(w)[-1])\n",
        "  return greedy_action(w,x,rs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Random policy (Exercise). \n",
        "env = GridWorld()\n",
        "last_x_w = env.reset()\n",
        "\n",
        "rs = np.random.RandomState(0)\n",
        "actions = np.array([0,1,2,3])\n",
        "for _ in range(1000):\n",
        "  last_action = random_action(actions, rs)\n",
        "  x_w, reward, done = env.step(last_action) \n",
        "\n",
        "  if done:\n",
        "    last_x_w = env.reset()\n",
        "  else:\n",
        "    last_x_w = x_w\n",
        "random_state_visitation = np.copy(env.counts)\n",
        "print(random_state_visitation)"
      ],
      "metadata": {
        "id": "pDW2del1Q5lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cec4915-8101-4be8-c368-cda182310774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[111.  75.  55.  43.  39.  62.  50.  37.  43.  38.  50.  41.  26.  21.\n",
            "  17.  48.  34.  26.  32.  28.  48.  40.  23.  14.  10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Greedy policy with random values (Exercise).\n",
        "env = GridWorld()\n",
        "last_x_w = env.reset()\n",
        "\n",
        "rs = np.random.RandomState(0)\n",
        "random_values = rs.rand(25,4)\n",
        "print(random_values)\n",
        "for _ in range(1000):\n",
        "  last_action = greedy_action(random_values, last_x_w, rs)\n",
        "  x_w, reward, done = env.step(last_action) \n",
        "\n",
        "  if done:\n",
        "    last_x_w = env.reset()\n",
        "  else:\n",
        "    last_x_w = x_w\n",
        "greedy_state_visitation = np.copy(env.counts)\n",
        "print(greedy_state_visitation)"
      ],
      "metadata": {
        "id": "0g7HJVfzRz3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc3a9b7-c47e-40fc-ad1d-3cbd5acc97bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5488135  0.71518937 0.60276338 0.54488318]\n",
            " [0.4236548  0.64589411 0.43758721 0.891773  ]\n",
            " [0.96366276 0.38344152 0.79172504 0.52889492]\n",
            " [0.56804456 0.92559664 0.07103606 0.0871293 ]\n",
            " [0.0202184  0.83261985 0.77815675 0.87001215]\n",
            " [0.97861834 0.79915856 0.46147936 0.78052918]\n",
            " [0.11827443 0.63992102 0.14335329 0.94466892]\n",
            " [0.52184832 0.41466194 0.26455561 0.77423369]\n",
            " [0.45615033 0.56843395 0.0187898  0.6176355 ]\n",
            " [0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            " [0.3595079  0.43703195 0.6976312  0.06022547]\n",
            " [0.66676672 0.67063787 0.21038256 0.1289263 ]\n",
            " [0.31542835 0.36371077 0.57019677 0.43860151]\n",
            " [0.98837384 0.10204481 0.20887676 0.16130952]\n",
            " [0.65310833 0.2532916  0.46631077 0.24442559]\n",
            " [0.15896958 0.11037514 0.65632959 0.13818295]\n",
            " [0.19658236 0.36872517 0.82099323 0.09710128]\n",
            " [0.83794491 0.09609841 0.97645947 0.4686512 ]\n",
            " [0.97676109 0.60484552 0.73926358 0.03918779]\n",
            " [0.28280696 0.12019656 0.2961402  0.11872772]\n",
            " [0.31798318 0.41426299 0.0641475  0.69247212]\n",
            " [0.56660145 0.26538949 0.52324805 0.09394051]\n",
            " [0.5759465  0.9292962  0.31856895 0.66741038]\n",
            " [0.13179786 0.7163272  0.28940609 0.18319136]\n",
            " [0.58651293 0.02010755 0.82894003 0.00469548]]\n",
            "[501. 500.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title $\\varepsilon$-Greedy policy with random values (Exercise).\n",
        "env = GridWorld()\n",
        "last_x_w = env.reset()\n",
        "\n",
        "rs = np.random.RandomState(0)\n",
        "random_values = rs.rand(25,4)\n",
        "for _ in range(1000):\n",
        "  last_action = epsilon_greedy(random_values, last_x_w, rs)\n",
        "  x_w, reward, done = env.step(last_action) \n",
        "\n",
        "  if done:\n",
        "    last_x_w = env.reset()\n",
        "  else:\n",
        "    last_x_w = x_w\n",
        "epsilon_greedy_state_visitation = np.copy(env.counts)\n",
        "print(epsilon_greedy_state_visitation)"
      ],
      "metadata": {
        "id": "ICZ8qB-MSAV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590ad7f6-bf14-46e6-d995-1c341959cbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[366. 350. 114.  19.  17.  28.  18.   8.   3.  14.   1.   0.   0.   1.\n",
            "  14.   1.  16.   1.   1.   0.  11.  16.   1.   1.   1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results with the code below."
      ],
      "metadata": {
        "id": "KB3W-w_5SSdT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xxEgTCG9SCJJ",
        "outputId": "5d6c5d37-f39d-42f5-cec8-6f7699acf470"
      },
      "source": [
        "plt.bar(np.arange(env.num_states), greedy_state_visitation, label='Greedy')\n",
        "plt.bar(np.arange(env.num_states), random_state_visitation, label='Random')\n",
        "plt.bar(np.arange(env.num_states), epsilon_greedy_state_visitation, label=r'$\\varepsilon$-Greedy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7faa28f60b90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYfUlEQVR4nO3df3BV1b338feXQATRyy8jA0QIvQVKLBgw/FLLQKKiVhE6/ihDMRIttSK90qelPnasPB20aq9XK6U8pY8KVBRELGAv9eqA1kLBEiSXX47CZYKEQX4KWiG9At/nj7PJDZiQk+ScnGSdz2smk73XXnvvtbKTz9lZe599zN0REZGwtEh1A0REJPEU7iIiAVK4i4gESOEuIhIghbuISIBaproBABdddJHn5OSkuhkiIs3Khg0bDrp7VnXLmkS45+TkUFJSkupmiIg0K2a2q6ZlGpYREQmQwl1EJEAKdxGRADWJMXcRCdMXX3xBeXk5FRUVqW5Ks9a6dWuys7Np1apV3Oso3EUkacrLy7nwwgvJycnBzFLdnGbJ3Tl06BDl5eX07Nkz7vU0LCMiSVNRUUGnTp0U7A1gZnTq1KnO//3EFe5mVmZmm82s1MxKorKOZvammW2PvneIys3MnjGzHWa2ycwG1rk3IhIMBXvD1ednWJcz95Hunufu+dH8A8BKd+8FrIzmAa4HekVfk4DZdW6ViIg0SEPG3G8GRkTT84C3gZ9E5fM99qD4dWbW3sy6uPvehjRURJq/nAf+PaHbK3vsm7XW2bdvH1OnTmXdunV06NCBzMxMpk2bxtixYxPaljvvvJMbb7yRW265JaHbra94w92BN8zMgd+6+xygc5XA/hjoHE13A3ZXWbc8Kjsj3M1sErEze7p3716/1hP/L0vVX4J41onnl0ZEmjZ3Z8yYMRQVFfHiiy8CsGvXLpYvX35GvRMnTtCyZVj3l8Q7LHOVuw8kNuQy2cyGV10YnaXX6SOd3H2Ou+e7e35WVrWPRhARaZBVq1aRmZnJPffcU1nWo0cPpkyZwty5cxk9ejQFBQUUFhby+eefU1xczODBgxkwYADLli0D4OTJk/z4xz9m0KBB9O/fn9/+9rdA7IXjvvvuo0+fPlx99dXs37+/cp9jxoyp3N+bb76Z8P8S4hHXS5W774m+7zezPwCDgX2nh1vMrAuwP6q+B7ikyurZUZmISKPaunUrAwfWfE/He++9x6ZNm+jYsSMPPvggBQUFPPfccxw5coTBgwdz9dVXs2DBAtq1a8f69ev5xz/+wZVXXsm1117Lxo0b+eCDD9i2bRv79u0jNzeX4uJiRo4cyb333suBAwfIysri+eefp7i4uBF7HVPrmbuZtTWzC09PA9cCW4DlQFFUrQhYFk0vB+6I7poZChzVeLuINAWTJ0/msssuY9CgQQBcc801dOzYEYA33niDxx57jLy8PEaMGEFFRQUfffQRb7zxBvPnzycvL48hQ4Zw6NAhtm/fzjvvvMO4cePIyMiga9euFBQUALE7WyZMmMALL7zAkSNHWLt2Lddff32j9zWeM/fOwB+iW3FaAi+6++tmth542czuAnYBt0X1VwA3ADuAY8DEhLdaRCQOl156KUuWLKmcnzVrFgcPHiQ/P3bTX9u2bSuXuTtLliyhT58+Z2zD3Zk5cyajRo06o3zFihU17nfixIncdNNNtG7dmltvvTUl4/m1nrm7+053vyz6utTdH4nKD7l7obv3cver3f1wVO7uPtnd/9nd+7m7nuUrIilRUFBARUUFs2f/zx3Zx44dq7buqFGjmDlzJrFLiLBx48bK8tmzZ/PFF18A8OGHH/L5558zfPhwFi1axMmTJ9m7dy9vvfVW5ba6du1K165dmTFjBhMnpub8NqzLwyLSpDX2XWhmxtKlS5k6dSpPPPEEWVlZtG3blscff5zjx4+fUfehhx7i/vvvp3///pw6dYqePXvyxz/+kbvvvpuysjIGDhyIu5OVlcXSpUsZO3Ysq1atIjc3l+7duzNs2LAztjd+/HgOHDhA3759G7PLlRTuIhK0Ll26sHDhwmqX3XnnnZXTbdq0qbwTpqoWLVrw6KOP8uijj35p2a9//esa97t69Wq++93v1r3BCaJwFxFJsMsvv5y2bdvy5JNPpqwNCncRkQTbsGFDqpugp0KKiIRI4S4iEiCFu4hIgBTuIiIB0gVVEWk809sleHtHa62SkZFBv379OHHiBD179uT3v/897du3b/Cu586dS0lJyTlvh0wlnbmLSNDatGlDaWkpW7ZsoWPHjsyaNSvVTWoUCncRSRvDhg1jz57YQ2r/9re/MWzYMAYMGMAVV1zBBx98AMTOyL/1rW9x3XXX0atXL6ZNm1a5/vPPP0/v3r0ZPHgwa9asqSwvKyujoKCA/v37U1hYyEcffQTE3iT1/e9/n6FDh/KVr3yFt99+m+LiYvr27XvGG6iSQeEuImnh5MmTrFy5ktGjRwPwta99jb/85S9s3LiRn//85zz44IOVdUtLS1m0aBGbN29m0aJF7N69m7179/Lwww+zZs0aVq9ezbZt2yrrT5kyhaKiIjZt2sT48eP5wQ9+ULnsk08+Ye3atTz11FOMHj2aqVOnsnXrVjZv3kxpaWnS+qsxdxEJ2vHjx8nLy2PPnj307duXa665BoCjR49SVFTE9u3bMbPKB4MBFBYW0q5d7PpAbm4uu3bt4uDBg4wYMYLTHy50++238+GHHwKwdu1aXn31VQAmTJhwxtn+TTfdhJnRr18/OnfuTL9+/YDYEyvLysrIy8tLSr915i4iQTs95r5r1y7cvXLM/aGHHmLkyJFs2bKF1157jYqKisp1zjvvvMrpjIwMTpw4Ue/9n95WixYtzthuixYtGrTd2ijcRSQtnH/++TzzzDM8+eSTnDhxgqNHj9KtWzcgNs5emyFDhvDnP/+ZQ4cO8cUXX7B48eLKZVdccUXlw8kWLFjAN77xjaT0oS40LCMijSeOWxeTacCAAfTv35+XXnqJadOmUVRUxIwZM/jmN2t/FHGXLl2YPn06w4YNo3379mcMp8ycOZOJEyfyy1/+svKj9VLNTj+YPpXy8/O9pKR+n+mR88C/x1Wv6nOk41mnsZ87LRKi999/P2XPMw9NdT9LM9vg7vnV1dewjIhIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiC9Q1VEGk2/ef0Sur3NRZsTtq19+/YxdepU1q1bR4cOHcjMzGTatGmMHTs2Yfs4bfr06VxwwQX86Ec/Svi2T9OZu4ikPXdnzJgxDB8+nJ07d7JhwwYWLlxIeXn5l+qdOnUqRa2sG4W7iATv9ddfJy8vj7y8PIYMGfKlgF61ahWZmZncc889lWU9evRgypQplJWV0adPH+644w6+/vWvs3v3bl544QUGDx5MXl4e3/ve9zh58mTlejUte+SRR+jduzdXXXVV5QeD/OxnP+Ppp5+uXPenP/0pv/rVrxLSZ4W7iARvypQp/OlPf6K0tJR3332XFi3OjL6tW7cycODAGtffvn079957L1u3buXYsWMsWrSINWvWUFpaSkZGBgsWLABiz3+pbtnp/wRKS0tZsWIF69evB6C4uJj58+cDcOrUKRYuXMh3vvOdhPRZY+4iErwbbriB/v37M378+DPOlGsyefJkVq9eTWZmJosXL6ZHjx4MHToUgJUrV7JhwwYGDRoExD4M5OKLLz7nssOHDzN27FjOP/98gMpPg8rJyaFTp05s3LiRffv2MWDAADp16pSQPivcRSRof/3rX3F39u7dS8uWscibNWsWv/vd7wBYsWIFl156KUuWLKlcZ9asWRw8eJD8/NgDF9u2bVu5zN0pKiriF7/4xZf2VdOyc72g3H333cydO5ePP/6Y4uLi+nf0LBqWEZGgLV68mN69e9OyZUvcnU8//ZTJkydTWlpKaWkpXbt2paCggIqKCmbPnl253rFjx6rdXmFhIa+88gr79+8H4PDhw+zateucy4YPH87SpUs5fvw4n332Ga+99lrl9saOHcvrr7/O+vXrGTVqVML6HfeZu5llACXAHne/0cx6AguBTsAGYIK7/7eZnQfMBy4HDgG3u3tZwlosIs1WIm9djNe4ceO46667mDNnDm3atOE3v/kNl19++Rl1zIylS5cydepUnnjiCbKysmjbti2PP/74l7aXm5vLjBkzuPbaazl16hStWrVi1qxZ9OjRo8ZlQ4cO5fbbb+eyyy7j4osvrhy2AcjMzGTkyJG0b9+ejIyMhPU77g/rMLMfAvnAP0Xh/jLwqrsvNLP/C/ynu882s3uB/u5+j5l9Gxjr7refa9v6sA6RMOnDOmp36tQpBg4cyOLFi+nVq1eN9ZLyYR1mlg18E/h/0bwBBcArUZV5wJho+uZonmh5YVRfRESq2LZtG1/96lcpLCw8Z7DXR7zDMk8D04ALo/lOwBF3P/3R3eVAt2i6G7AbwN1PmNnRqP7Bqhs0s0nAJIDu3bvXt/0iIs1Wbm4uO3fuTMq2az1zN7Mbgf3uviGRO3b3Oe6e7+75WVlZidy0iEjai+fM/UpgtJndALQG/gn4FdDezFpGZ+/ZwJ6o/h7gEqDczFoC7YhdWBURkUZS65m7u/9vd8929xzg28Aqdx8PvAXcElUrApZF08ujeaLlqzzeq7YiEhz9+TdcfX6GDbnP/SfAD81sB7Ex9Wej8meBTlH5D4EHGrAPEWnGWrduzaFDhxTwDeDuHDp0iNatW9dpvTq9Q9Xd3wbejqZ3AoOrqVMB3FqnVohIkLKzsykvL+fAgQOpbkqz1rp1a7Kzs+u0jh4/ICJJ06pVK3r27JnqZqQlPX5ARCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJULN/E9OFfeN9uoE+fENE0ofO3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAtTs75apj/jusNHdNSLSfOnMXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQLWGu5m1NrO/mdl/mtlWM/s/UXlPM3vXzHaY2SIzy4zKz4vmd0TLc5LbBREROVs8Z+7/AArc/TIgD7jOzIYCjwNPuftXgU+Au6L6dwGfROVPRfVERKQR1RruHvP3aLZV9OVAAfBKVD4PGBNN3xzNEy0vNDNLWItFRKRWcY25m1mGmZUC+4E3gf8Cjrj7iahKOdAtmu4G7AaIlh8FOlWzzUlmVmJmJQcOHGhYL0RE5Axxhbu7n3T3PCAbGAx8raE7dvc57p7v7vlZWVkN3ZyIiFRRp7tl3P0I8BYwDGhvZi2jRdnAnmh6D3AJQLS8HXAoIa0VEZG4xHO3TJaZtY+m2wDXAO8TC/lbompFwLJoenk0T7R8lbt7IhstIiLn1rL2KnQB5plZBrEXg5fd/Y9mtg1YaGYzgI3As1H9Z4Hfm9kO4DDw7SS0W0REzqHWcHf3TcCAasp3Eht/P7u8Arg1Ia0TEZF60TtURUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRANUa7mZ2iZm9ZWbbzGyrmf1LVN7RzN40s+3R9w5RuZnZM2a2w8w2mdnAZHdCRETOFM+Z+wngf7l7LjAUmGxmucADwEp37wWsjOYBrgd6RV+TgNkJb7WIiJxTreHu7nvd/b1o+jPgfaAbcDMwL6o2DxgTTd8MzPeYdUB7M+uS8JaLiEiN6jTmbmY5wADgXaCzu++NFn0MdI6muwG7q6xWHpWdva1JZlZiZiUHDhyoY7NFRORc4g53M7sAWALc7+6fVl3m7g54XXbs7nPcPd/d87OysuqyqoiI1CKucDezVsSCfYG7vxoV7zs93BJ93x+V7wEuqbJ6dlQmIiKNJJ67ZQx4Fnjf3f+tyqLlQFE0XQQsq1J+R3TXzFDgaJXhGxERaQQt46hzJTAB2GxmpVHZg8BjwMtmdhewC7gtWrYCuAHYARwDJia0xSIiUqtaw93dVwNWw+LCauo7MLmB7RIRkQbQO1RFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCVDLVDegOeg3r19c9TYXbU5yS0RE4qMzdxGRANUa7mb2nJntN7MtVco6mtmbZrY9+t4hKjcze8bMdpjZJjMbmMzGi4hI9eI5c58LXHdW2QPASnfvBayM5gGuB3pFX5OA2YlppoiI1EWt4e7u7wCHzyq+GZgXTc8DxlQpn+8x64D2ZtYlUY0VEZH41HfMvbO7742mPwY6R9PdgN1V6pVHZV9iZpPMrMTMSg4cOFDPZoiISHUafEHV3R3weqw3x93z3T0/Kyuroc0QEZEq6hvu+04Pt0Tf90fle4BLqtTLjspERKQR1TfclwNF0XQRsKxK+R3RXTNDgaNVhm9ERKSR1PomJjN7CRgBXGRm5cDDwGPAy2Z2F7ALuC2qvgK4AdgBHAMmJqHNIiJSi1rD3d3H1bCosJq6DkxuaKNERKRh9A5VEZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKkD8hurqa3i6PO0eS3Q0SaJJ25i4gESOEuIhIghbuISIAU7iIiAdIF1XQRzwVYaNoXYUPog0gjUbiLNGXp+oKWrv1OIA3LiIgESGfuyZCOZx3p2GeRJkzhLuHSC0760LH+EoV7U5Cuv5jp2m+RRqBwl+opeEWaNYW7SGj03CFB4S4idaX/6poFhXsz1a9n91rrbG6EdojUSi8GKaFwF6kqHYc0FL5BUriLSPpJgxc0hbtIQ6RBSEjzpHBPgnjGw6Fxx8SbYptEJHkU7k2AgjeNNMKZfjpebNff0Jcp3CVl0vEPMh37HIxmdrFd4S4iTY5eBBtO4S7V0h+XSAOl+GJ7Up7nbmbXmdkHZrbDzB5Ixj5ERKRmCQ93M8sAZgHXA7nAODPLTfR+RESkZskYlhkM7HD3nQBmthC4GdiWhH1JE9EYwzhNcago2W1qij/Xpngc6qo+fajrXUip/jmZuyd2g2a3ANe5+93R/ARgiLvfd1a9ScCkaLYP8EECm3ERcDCB22su0rHf6nP6SMd+19bnHu6eVd2ClF1Qdfc5wJxkbNvMStw9PxnbbsrSsd/qc/pIx343pM/JuKC6B7ikynx2VCYiIo0kGeG+HuhlZj3NLBP4NrA8CfsREZEaJHxYxt1PmNl9wH8AGcBz7r410fupRVKGe5qBdOy3+pw+0rHf9e5zwi+oiohI6iXlTUwiIpJaCncRkQAFF+7p+OgDMyszs81mVmpmJaluT7KY2XNmtt/MtlQp62hmb5rZ9uh7h1S2MdFq6PN0M9sTHe9SM7shlW1MNDO7xMzeMrNtZrbVzP4lKg/2WJ+jz/U+1kGNuUePPvgQuAYoJ3bnzjh3D/rdsWZWBuS7e9Bv8DCz4cDfgfnu/vWo7AngsLs/Fr2Yd3D3n6SynYlUQ5+nA393939NZduSxcy6AF3c/T0zuxDYAIwB7iTQY32OPt9GPY91aGfulY8+cPf/Bk4/+kAC4O7vAIfPKr4ZmBdNzyP2BxGMGvocNHff6+7vRdOfAe8D3Qj4WJ+jz/UWWrh3A3ZXmS+ngT+gZsKBN8xsQ/RYh3TS2d33RtMfA51T2ZhGdJ+ZbYqGbYIZnjibmeUAA4B3SZNjfVafoZ7HOrRwT1dXuftAYk/inBz9K592PDbGGM44Y81mA/8M5AF7gSdT25zkMLMLgCXA/e7+adVloR7ravpc72MdWrin5aMP3H1P9H0/8Adiw1PpYl80Xnl63HJ/ituTdO6+z91Puvsp4HcEeLzNrBWxkFvg7q9GxUEf6+r63JBjHVq4p92jD8ysbXQBBjNrC1wLbDn3WkFZDhRF00XAshS2pVGcDrjIWAI73mZmwLPA++7+b1UWBXusa+pzQ451UHfLAES3Cj3N/zz64JEUNympzOwrxM7WIfY4iRdD7bOZvQSMIPYY1H3Aw8BS4GWgO7ALuM3dg7kAWUOfRxD7N92BMuB7Vcaimz0zuwr4C7FHnZ+Kih8kNgYd5LE+R5/HUc9jHVy4i4hIeMMyIiKCwl1EJEgKdxGRACncRUQCpHAXEQmQwl1EJEAKdxGRAP1/5mbvB6qYcqkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations (Exercise)\n",
        "WRITE YOUR OBSERVATIONS HERE."
      ],
      "metadata": {
        "id": "4gy63xclSYLQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjXZwmsSWXLm"
      },
      "source": [
        "# Part 2. Policy Evaluation (Prediction)\n",
        "\n",
        "When performing policy evalution, a system learns to predict the sum of future rewards that occur while following a policy $\\pi$. If the experience used to learn the prediction comes from the policy itself, then we say the evaluation is on-policy. In this component you will implement Sarsa, a learning algorithm for on-policy evaluation. \n",
        "\n",
        "The Sarsa algorithm starts with an arbitrary estimate of action values $q_\\pi(s,a)$ for all $s,a$, then it uses experience gathered under $\\pi$ to incrementally improve the action-value estimates. The update rule follows the method of temporal differences at each transition $s,a\\rightarrow r,s',a'$, where $a,a'\\sim \\pi$. See section 6.4 of Sutton and Barto for more information.\n",
        "$$q_\\pi(s,a) \\gets q_\\pi(s,a) + \\alpha[r +\\gamma q_\\pi(s',a') - q_\\pi(s,a)].$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4Z29cyJp4I",
        "cellView": "form"
      },
      "source": [
        "#@title Visualization code\n",
        "def plot_colored_value_cell(x, y, q, ax, norm):\n",
        "  scale = 0.2\n",
        "  size = scale * 1\n",
        "  cmap = cm.get_cmap('jet')#cm.Jet\n",
        "  v = np.max(q)\n",
        "  color=cmap(norm(v)) \n",
        "  head_width = .05 * scale\n",
        "  head_length = .05 * scale\n",
        "  x = scale * x\n",
        "  y = scale * y\n",
        "\n",
        "  rect = plt.Rectangle([x, y], size, size,\n",
        "                      facecolor=color, \n",
        "                      edgecolor='black',\n",
        "                      clip_on=False)\n",
        "  ax.add_patch(rect)\n",
        "\n",
        "  ax.text(x+size/2, y+size/2, '{value:.2f}'.format(value=v))\n",
        "\n",
        "  return ax\n",
        "\n",
        "def plot_arrow_value_cell(x, y, q, ax):\n",
        "  scale = 0.2\n",
        "  size = scale * 1\n",
        "\n",
        "  color='white'\n",
        "  head_width = .05 * scale\n",
        "  head_length = .05 * scale\n",
        "  q_norm = scale * .5 * q / np.sqrt(np.sum(q**2)) \n",
        "  x = scale * x\n",
        "  y = scale * y\n",
        "\n",
        "  rect = plt.Rectangle([x, y], size, size,\n",
        "                      facecolor=color, \n",
        "                      edgecolor='black',\n",
        "                      clip_on=False)\n",
        "  ax.add_patch(rect)\n",
        "  x = x + .5 * scale\n",
        "  y = y + .5 * scale\n",
        "\n",
        "  # left\n",
        "  dx = q_norm[0]\n",
        "  ax.arrow(x, y, -dx, 0, \n",
        "            length_includes_head=True,\n",
        "            head_width=head_width,\n",
        "            head_length=head_length,\n",
        "            color='black')\n",
        "  # up\n",
        "  dy = q_norm[1]\n",
        "  ax.arrow(x, y, 0, dy, \n",
        "            length_includes_head=True,\n",
        "            head_width=head_width,\n",
        "            head_length=head_length,\n",
        "            color='black')\n",
        "  # right\n",
        "  dx = q_norm[2]\n",
        "  ax.arrow(x, y, dx, 0, \n",
        "            length_includes_head=True,\n",
        "            head_width=head_width,\n",
        "            head_length=head_length,\n",
        "            color='black')\n",
        "  #down\n",
        "  dy = q_norm[3]\n",
        "  ax.arrow(x, y, 0, -dy, \n",
        "            length_includes_head=True,\n",
        "            head_width=head_width,\n",
        "            head_length=head_length,\n",
        "            color='black')\n",
        "  return ax\n",
        "\n",
        "def plot_greedy_cell(x, y, q, ax):\n",
        "  scale = 0.2\n",
        "  size = scale * 1\n",
        "\n",
        "  color='white'\n",
        "  head_width = .05 * scale\n",
        "  head_length = .05 * scale\n",
        "  q_norm = scale * .5 * q / np.sqrt(np.sum(q**2)) \n",
        "  i_greedy = np.argmax(q_norm)\n",
        "\n",
        "  x = scale * x\n",
        "  y = scale * y\n",
        "\n",
        "  rect = plt.Rectangle([x, y], size, size,\n",
        "                      facecolor=color, \n",
        "                      edgecolor='black',\n",
        "                      clip_on=False)\n",
        "  ax.add_patch(rect)\n",
        "  x = x + .5 * scale\n",
        "  y = y + .5 * scale\n",
        "\n",
        "  if i_greedy == 0:\n",
        "    # left\n",
        "    dx = q_norm[0]\n",
        "    ax.arrow(x, y, -dx, 0, \n",
        "              length_includes_head=True,\n",
        "              head_width=head_width,\n",
        "              head_length=head_length,\n",
        "              color='black')\n",
        "  elif i_greedy == 1:\n",
        "    # up\n",
        "    dy = q_norm[1]\n",
        "    ax.arrow(x, y, 0, dy, \n",
        "              length_includes_head=True,\n",
        "              head_width=head_width,\n",
        "              head_length=head_length,\n",
        "              color='black')\n",
        "  elif i_greedy == 2:\n",
        "    # right\n",
        "    dx = q_norm[2]\n",
        "    ax.arrow(x, y, dx, 0, \n",
        "              length_includes_head=True,\n",
        "              head_width=head_width,\n",
        "              head_length=head_length,\n",
        "              color='black')\n",
        "  elif i_greedy == 3:\n",
        "    #down\n",
        "    dy = q_norm[3]\n",
        "    ax.arrow(x, y, 0, -dy, \n",
        "              length_includes_head=True,\n",
        "              head_width=head_width,\n",
        "              head_length=head_length,\n",
        "              color='black')\n",
        "  return ax\n",
        "\n",
        "\n",
        "def plot_qvals(env, agent, arrow_plot=False):\n",
        "  qvalues = np.zeros((env.num_states, env.num_actions))\n",
        "  for s in range(env.num_states):\n",
        "    state = jax.nn.one_hot(s, env.num_states)\n",
        "    for a in range(env.num_actions):\n",
        "      qvalues[s, a] = agent.value(state, a)\n",
        "  \n",
        "  vmin, vmax = np.min(qvalues), np.max(qvalues)\n",
        "  norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax, clip=True)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "  for i in range(25):\n",
        "    x, y = env.get_xy(i)\n",
        "    if arrow_plot:\n",
        "      plot_arrow_value_cell(x, y, qvalues[i], ax)\n",
        "    else:\n",
        "      plot_colored_value_cell(x, y, qvalues[i], ax, norm)\n",
        "  ax.set_axis_off()\n",
        "  plt.plot()\n",
        "\n",
        "\n",
        "def plot_greedy_policy(env, agent):\n",
        "  qvalues = np.zeros((env.num_states, env.num_actions))\n",
        "  for s in range(env.num_states):\n",
        "    state = jax.nn.one_hot(s, env.num_states)\n",
        "    for a in range(env.num_actions):\n",
        "      qvalues[s, a] = agent.value(state, a)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "  for i in range(25):\n",
        "    x, y = env.get_xy(i)\n",
        "    plot_greedy_cell(x, y, qvalues[i], ax)\n",
        "  \n",
        "  ax.set_axis_off()\n",
        "  plt.plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Exercise: Sarsa Implementation.\n",
        "\n",
        " Implement the Sarsa algorithm."
      ],
      "metadata": {
        "id": "lt1fmHpcKVMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sarsa (Exercise)\n",
        "class Sarsa(object):\n",
        "  \"\"\"Sarsa.\"\"\"\n",
        "  def __init__(self,\n",
        "               first_x_w,\n",
        "               num_actions,\n",
        "               discount=0.9,\n",
        "               alpha_w=1.):\n",
        "    self.last_x_w = first_x_w\n",
        "    self.num_actions = num_actions\n",
        "    self.w_q = jnp.zeros((len(self.last_x_w), self.num_actions))\n",
        "    self.discount = discount\n",
        "    self.alpha_w= alpha_w\n",
        "\n",
        "  def update(self, last_x_w, last_a, reward, x_w, a, done):\n",
        "    # YOUR CODE GOES HERE.\n",
        "    if not done:\n",
        "      self.w_q[np.argmax(last_x_w), last_a] = self.w_q[np.argmax(last_x_w), last_a] + self.alpha_w(reward + self.discount*self.w_q[np.argmax(x_w), a] - self.w_q[np.argmax(last_x_w), last_a]) \n",
        "\n",
        "    if done:\n",
        "      self.w_q[np.argmax(last_x_w), last_a] = self.w_q[np.argmax(last_x_w), last_a]\n",
        "  def value(self, x_w, action):\n",
        "    # YOUR CODE GOES HERE.\n",
        "    return "
      ],
      "metadata": {
        "id": "POTdnkcuSpqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "31a030a0-07ca-4ada-f023-918db72762e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-05b90e5c4def>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    def value(self, x_w, action):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58OXLcEOuRxw"
      },
      "source": [
        "## Exercise: Policy evaluation with Sarsa.\n",
        "\n",
        "Use your implementation of Sarsa to evaluate the actions of a random policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCzbjwWWaXVf"
      },
      "source": [
        "env = GridWorld()\n",
        "last_x_w = env.reset()\n",
        "agent = Sarsa(first_x_w=last_x_w, num_actions=env.num_actions)\n",
        "\n",
        "rs = np.random.RandomState(0)\n",
        "actions = np.arange(env.num_actions)\n",
        "steps = []\n",
        "ep_steps = 0.\n",
        "for t in range(1000):\n",
        "  last_action = random_action(actions, rs)\n",
        "  x_w, reward, done = env.step(last_action) \n",
        "  action = random_action(actions, rs)\n",
        "  agent.update(last_x_w, last_action, reward, x_w, action, done)\n",
        "\n",
        "  ep_steps += 1\n",
        "  if done:\n",
        "    last_x_w = env.reset()\n",
        "    steps.append(ep_steps)\n",
        "    ep_steps = 0.\n",
        "  else:\n",
        "    last_x_w = x_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAPvpX1KvvzO"
      },
      "source": [
        "plot_qvals(env, agent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvwlEaxLuWg6"
      },
      "source": [
        "## Exercise: On-policy control with Sarsa.\n",
        "\n",
        "Sarsa can also be used as an on-policy control algorithm. One case in which this occurs is under the $\\varepsilon$-greedy policy. \n",
        "\n",
        "At any given time, the learner selects the best action according to the current estimate of the action-value function, with probability $1-\\varepsilon$. Then $\\varepsilon$ percent of the time the learner acts randomly. \n",
        "\n",
        "Throughout this process, the values that Sarsa learns become increasingly accurate. This causes the policy that it evaluates to change. Eventually, the process stabilizes (at least in this domain) when the values reflect the true greedy actions. These will be optimal for achieving high return.\n",
        "\n",
        "Use your implementation of Sarsa to evaluate the actions of an epsilon-greedy policy. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4TtMxmZuV9x"
      },
      "source": [
        "env = GridWorld()\n",
        "last_x_w = env.reset()\n",
        "agent = Sarsa(first_x_w=last_x_w, num_actions=env.num_actions)\n",
        "\n",
        "rs = np.random.RandomState(0)\n",
        "steps = []\n",
        "ep_steps = 0.\n",
        "for t in range(10000):\n",
        "  last_action = epsilon_greedy(agent.w_q, last_x_w, rs)\n",
        "  x_w, reward, done = env.step(last_action) \n",
        "  action = epsilon_greedy(agent.w_q, x_w, rs)\n",
        "  agent.update(last_x_w, last_action, reward, x_w, action, done)\n",
        "\n",
        "  ep_steps += 1\n",
        "  if done:\n",
        "    last_x_w = env.reset()\n",
        "    steps.append(ep_steps)\n",
        "    ep_steps = 0.\n",
        "  else:\n",
        "    last_x_w = x_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cii5_efbPLq"
      },
      "source": [
        "plot_qvals(env, agent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h1YnWvoaCI_"
      },
      "source": [
        "# Part 3. Policy Iteration (Control)\n",
        "\n",
        "Reinforcement learning is renowed for having general-purpose algorithms that can learn useful decision policies without access to a transition model. Algorithms of this type sample transitions by directly interacting with an environment: taking actions and observing the outcomes. With this feedback, a learning system can iteratively evaluate and improve its policy, so that in the long term, its actions lead to the maximum amount of future reward.\n",
        "\n",
        "Here we focus on an off-policy algorithm for control. Off-policy algorithms use experience gathered under a behavior policy, $\\pi$, to update a target policy $\\pi^*$. The Q-Learning algorithm (Watkins 1989) considers the greedy target policy and applies the method of temporal differences to update its action-value estimates. The update rule is given below.\n",
        "\n",
        "\n",
        "$$q(s,a) \\gets q(s,a) + \\alpha[r+\\gamma \\max_{a'\\in\\mathcal{A}} q(s',a') - q(s,a)].$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICY4mlbVPgW8"
      },
      "source": [
        " ## Exercise: Q-Learning implementation\n",
        "\n",
        "Implement Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Q-Learning (Exercise)\n",
        "class QLearning(object):\n",
        "  \"\"\"QLearning.\"\"\"\n",
        "  def __init__(self,\n",
        "               first_x_w,\n",
        "               num_actions,\n",
        "               discount=0.9,\n",
        "               alpha_w=1.):\n",
        "    self.last_x_w = first_x_w\n",
        "    self.num_actions = num_actions\n",
        "    self.w_q = jnp.zeros((len(self.last_x_w), self.num_actions))\n",
        "    self.discount = discount\n",
        "    self.alpha_w= alpha_w\n",
        "\n",
        "  def update(self, x_w, last_a, reward, done):\n",
        "    # YOUR CODE GOES HERE.\n",
        "\n",
        "  def value(self, x_w, action):\n",
        "    # YOUR CODE GOES HERE.\n",
        "    return "
      ],
      "metadata": {
        "cellView": "form",
        "id": "mDYi3zD2UMH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqRdIoinVYTd"
      },
      "source": [
        "env = GridWorld()\n",
        "last_x_w = env.reset()\n",
        "agent = QLearning(first_x_w=last_x_w, num_actions=env.num_actions)\n",
        "\n",
        "rs = np.random.RandomState(0)\n",
        "steps = []\n",
        "ep_steps = 0.\n",
        "for t in range(1000):\n",
        "  last_action = epsilon_greedy(agent.w_q, last_x_w, rs)\n",
        "  x_w, reward, done = env.step(last_action) \n",
        "  agent.update(last_x_w, last_action, reward, x_w, done)\n",
        "\n",
        "  ep_steps += 1\n",
        "  if done:\n",
        "    last_x_w = env.reset()\n",
        "    steps.append(ep_steps)\n",
        "    ep_steps = 0.\n",
        "  else:\n",
        "    last_x_w = x_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChnL7G1TwUKZ"
      },
      "source": [
        "plot_qvals(env, agent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2tOT6PMDOS7"
      },
      "source": [
        "plot_greedy_policy(env, agent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqOl-Q3Eqgfx"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.plot(np.arange(len(steps)), steps, label='Q-Learning')\n",
        "ax.hlines(y=8, xmin=0, xmax=len(steps), label='Shortest path')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Steps to Goal')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4MbWWQTJoCk"
      },
      "source": [
        " ## Exercise: Exploration parameter sensitivity.\n",
        "\n",
        "The parameter $\\varepsilon$ is said to control the amount of exploration the learning system experiences. In this context, $\\varepsilon$ specifically controls the probability of taking an action uniformly at random. \n",
        "\n",
        "In this exercise, we would like to tune the exploration parameter to acheive better learning performance. Choose an aggregate performance metric, such as the average number of steps to the goal, then evaluate the metric for several different values of $\\varepsilon$, e.g. $\\varepsilon \\in \\{0, 0.1, 0.2, 0.3, \\cdots, 1\\}$. What value of $\\varepsilon$ achieved the best aggregate performance? Construct a sensitivy curve by plotting $\\varepsilon$ versus aggregate performance. Comment on the profile of the curve; explain the behavior at the extreme values. Do you believe that further improvements would be possible with a different evaluation? "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "\n",
        "# YOUR CODE GOES HERE."
      ],
      "metadata": {
        "id": "T7V8kgU5VRBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzBuIebpSlfu"
      },
      "source": [
        " ## Exercise: Step-size sensitivity.\n",
        "The stepsize $\\alpha$ controls the size of the learning system's corrections. This can have a significant effect on the system's overall adaptibility: its rate of learning and its longterm stability. \n",
        "\n",
        "In this exercise we would like you to fix the exploration parameter to $\\varepsilon =0.1$ and repeat the previous sensitivity analysis for the step size. Using the step size in place of the exploration parameter, use the same aggregate performance metric to evaluate several different values of $\\alpha$. For example, make sure your evaluation range captures the extreme cases of $\\alpha=0$ and $\\alpha=1$, e.g. $\\alpha\\in\\{0,10^{-3}, 10^{-2}, 10^{-1}, 1\\}$. Which value of $\\alpha$ demonstrated the highest aggregate performance? \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "\n",
        "# YOUR CODE GOES HERE."
      ],
      "metadata": {
        "cellView": "form",
        "id": "8sKhdU6DWvTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awy4bnFfSGf_"
      },
      "source": [
        "## Exercise: Stochastic transitions.\n",
        "The previous exercises took place in a deterministic environment. In reality, most environments are stochastic, and these are also some of the most interesting application domains of RL.\n",
        "\n",
        "Point out the differences from the previously studeied phenomena. This should have been observed in your results from the previous two exercises. Repeat the above analysis for the stochastic domain. How did the results change?\n",
        "\n",
        "Now that the domain is stochastic, it does not suffice to look at the performance of just a single trial. Run 30 trials with different seeds and plot the average performance as well as the standard error confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "\n",
        "# YOUR CODE GOES HERE."
      ],
      "metadata": {
        "cellView": "form",
        "id": "uShYtuvMWxS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the code below to plot learning performance."
      ],
      "metadata": {
        "id": "RA0DyoUsW6ON"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY2YGEe6JbBm"
      },
      "source": [
        "def mean_confidence_interval(data):\n",
        "    import scipy as sp \n",
        "    \"\"\"Compute confidence interval from sample data.\"\"\"\n",
        "    num_samp = len(data)\n",
        "    mean = np.array(np.mean(data, axis=0))\n",
        "    se = sp.stats.sem(data, axis=0)\n",
        "    return mean, se"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0nfng0GIwja"
      },
      "source": [
        "data = np.array(trial_steps)\n",
        "mean_steps, se = mean_confidence_interval(data)\n",
        "plt.errorbar(np.arange(len(mean_steps)), mean_steps, yerr=se)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpdNkFd-hG-w"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.plot(np.arange(len(steps)), steps, label='Q-Learning')\n",
        "ax.hlines(y=8, xmin=0, xmax=len(steps), label='Shortest path')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Steps to Goal')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4. Continuous control with PPO.\n",
        "\n",
        "This next part with guide you through a control problem using the PPO algorithm from the [Stable Baselines repository](https://github.com/hill-a/stable-baselines). \n",
        "\n",
        "The code is adapted from the [Stable Baselines tutorial](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/stable_baselines_getting_started.ipynb)."
      ],
      "metadata": {
        "id": "ut1nYK60rcjG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S-rFGcQmSlE"
      },
      "source": [
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to create a RL model, train it and evaluate it.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "```\n",
        "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines[mpi]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.2\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U29X1-B-AIKE"
      },
      "outputs": [],
      "source": [
        "import stable_baselines\n",
        "stable_baselines.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines works on environments that follow the [gym interface](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environments [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines.readthedocs.io/en/master/guide/algos.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines import PPO2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "You need to import the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO2('MlpPolicy', env)``` instead of ```PPO2(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy` (different from `stable_baselines.common.policies.MlPolicy`), that's why using string for the policy is the recommened option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines.common.policies import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright.\n",
        "\n",
        "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "Note: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the `DummyVecEnv`.\n",
        "\n",
        "We chose the `MlpPolicy` because input of `CartPole` is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) algorithm (PPO2 is the version optimized for GPU), which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWOAFJjumSlL"
      },
      "source": [
        "## Exercise: Sweeps of architecture size and step size.\n",
        "\n",
        "> `verbose=1` prints the output as the model learns\n",
        "\n",
        "> `learning_rate`: (float or callable) The step size, it can be a function. See\n",
        "https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/ppo2/ppo2.py\n",
        "for further details.\n",
        "\n",
        "> `net_arch`: parameter allows to specify the amount and size of the hidden layers \n",
        "and how many of them are shared between the policy network and the value \n",
        "network. See \n",
        "https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/common/policies.py\n",
        "for further details. \n",
        "\n",
        "**Adjust the step size and architecture** and observe the differences in learning performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# vectorized environments allow to easily multiprocess training\n",
        "# we demonstrate its usefulness in the next examples\n",
        "# env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# model = PPO2(MlpPolicy, env, verbose=0)\n",
        "model = PPO2(MlpPolicy, env, verbose=0, \n",
        "             learning_rate=2.5e-4,\n",
        "             policy_kwargs=dict(net_arch=[64, 64]))\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b20zH8BSvxnN"
      },
      "outputs": [],
      "source": [
        "print(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "We create a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hkyafs--gJz"
      },
      "source": [
        "In fact, Stable-Baselines already provides you with that helper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ZNldIR-fce"
      },
      "outputs": [],
      "source": [
        "from stable_baselines.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "# Use a separate environment for evaluation\n",
        "eval_env = gym.make('CartPole-v1')\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvQYCOMmSlQ"
      },
      "source": [
        "The `mean_reward` above is quite low for this task. Your challenge in this exercise is:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "## Exercise: Try and adjust the parameters above to get the highest possible mean reward.\n",
        "* What parameter combinations worked well?\n",
        "* What does the performance do as you keep changing a parameter in one direction? For example, try running with:\n",
        "`learning_rate: 1e-6, 1e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1`.\n",
        "* Try changing the `arch` to be more/less powerful. How does this impact performance? Does it saturate? You can try using the colab magic `%%timeit` to see the impact on training speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpOhTaImSlR"
      },
      "source": [
        "## `STOP HERE` \n",
        "If, after the session, you want to try and visualise the models, you can try and run the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "## Prepare video recording\n",
        "\n",
        "**Note:** This will _NOT_ work on a Google Colaboratory. If you run this locally, you can try and get this working. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "outputs": [],
      "source": [
        "record_video('CartPole-v1', model, video_length=500, prefix='ppo2-cartpole')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "outputs": [],
      "source": [
        "show_videos('videos', prefix='ppo2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wR_X8o3HbNB"
      },
      "source": [
        "# Want to learn more?\n",
        "\n",
        "Many of these references were originally compiled by Feryal Behbahani and Gheorghe Comanici [[here](https://colab.research.google.com/github/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb#scrollTo=XWqlIWbwN7Mk)].\n",
        "\n",
        "Books and lecture notes\n",
        "*   [Reinforcement Learning: an Introduction by Sutton & Barto](http://incompleteideas.net/book/RLbook2018.pdf)\n",
        "* [Algorithms for Reinforcement Learning by Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n",
        "\n",
        "Lectures and course \n",
        "*   [Reinforcement learning course at UCL by David Silver](https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-)\n",
        "*   [Reinforcement learning course by UCL & DeepMind](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb)\n",
        "*   [Reinforcement learning course at Stanford by Emma Brunskill](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)\n",
        "*   [Reinforcement learning course on U Alberta Coursera by Martha White & Adam White](https://www.coursera.org/specializations/reinforcement-learning)\n",
        "*   [Reinforcement learning lecture at NPTEL by Balaraman Ravindran](https://youtu.be/YaPSPu7K9S0)\n",
        "\n",
        "More practical:\n",
        "* [Spinning Up in Deep RL by Josh Achiam](https://spinningup.openai.com/en/latest/)\n",
        "* [An interactive tutorial on TD learning by Andrej Karpathy](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html)\n",
        "\n",
        "\n"
      ]
    }
  ]
}